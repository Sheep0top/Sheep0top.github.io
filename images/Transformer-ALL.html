

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Sheep0">
  <meta name="keywords" content="">
  
    <meta name="description" content="一、Tokenization将原始文本拆分成一个个token 1. 字粒度对于英语：按字母分割 对于中文：按字分割 优点：词表小 缺点：丢失了大量词汇的语义信息与边界信息、难以训练、对计算也会带来压力（输入输出变多） 2. 词粒度对于英语等拉丁语言：按空格切分 对于中文：前（后）向最大匹配法、最短路径分词法、基于 N-gram 的统计词频分词法 (1) 前向最大匹配法选定最大匹配长度，例如2（可以">
<meta property="og:type" content="website">
<meta property="og:title" content="page.title">
<meta property="og:url" content="https://blog.sheep0.top/images/Transformer-ALL.html">
<meta property="og:site_name" content="Sheep0&#39;s blog">
<meta property="og:description" content="一、Tokenization将原始文本拆分成一个个token 1. 字粒度对于英语：按字母分割 对于中文：按字分割 优点：词表小 缺点：丢失了大量词汇的语义信息与边界信息、难以训练、对计算也会带来压力（输入输出变多） 2. 词粒度对于英语等拉丁语言：按空格切分 对于中文：前（后）向最大匹配法、最短路径分词法、基于 N-gram 的统计词频分词法 (1) 前向最大匹配法选定最大匹配长度，例如2（可以">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.sheep0.top/images/04e1aa31e0fe35d358b00f54380e70b3.png">
<meta property="og:image" content="https://blog.sheep0.top/images/5d0e6ceac151b265fb6f9eb622edcd45.png">
<meta property="og:image" content="https://blog.sheep0.top/f5ca8bd92ac8fcc749ed9ff70118632c.png">
<meta property="og:image" content="https://blog.sheep0.top/images/1fbdd0a92d847ea07dddc63f1d97cd72.png">
<meta property="og:image" content="https://blog.sheep0.top/images/24533fde8728304e988456cd0d070cdf.png">
<meta property="og:image" content="https://blog.sheep0.top/images/601f006b098e0b189dbed624bf0a1fbe.png">
<meta property="og:image" content="https://blog.sheep0.top/images/10df3d75326e08b21120e9abe81ec092.png">
<meta property="og:image" content="https://blog.sheep0.top/images/7f9dc8d5dcce86802cd09fdb8cfab813.png">
<meta property="og:image" content="https://blog.sheep0.top/images/de3203e8c626fb32bc05c8cc1f15fc6b.png">
<meta property="og:image" content="https://blog.sheep0.top/images/8fb17bf5d3312a80d7c56861846ed6a3.png">
<meta property="og:image" content="https://blog.sheep0.top/images/92d1d2d0c1dec2c007b79660bd006b70.png">
<meta property="og:image" content="https://blog.sheep0.top/images/fcc0195f38f9c59efbad3575ca8840c9.png">
<meta property="og:image" content="https://blog.sheep0.top/images/887e6854a1bf5d78850a613f773eecdb.png">
<meta property="og:image" content="https://blog.sheep0.top/images/cdb4fb94d99db597f35c0a67be6b9d54.png">
<meta property="og:image" content="https://blog.sheep0.top/images/aa319f8e3ace183e660e4642abf5ddb7.png">
<meta property="og:image" content="https://blog.sheep0.top/images/cb7380500a802c0d94b93d1605fd799c.png">
<meta property="og:image" content="https://blog.sheep0.top/images/b176a81741ba6549ea2f1908584bbbd7.png">
<meta property="og:image" content="https://blog.sheep0.top/images/45e0593eae6b9813290bc1a589ba34a1.png">
<meta property="og:image" content="https://blog.sheep0.top/images/bb90ed7e104f1a65b120fd135ad70521.png">
<meta property="og:image" content="https://blog.sheep0.top/images/46e67535c49be6125525d986521f2b03.png">
<meta property="og:image" content="https://blog.sheep0.top/images/90a8eb7cae25f02eb8ebceafac55d57e.png">
<meta property="og:image" content="https://blog.sheep0.top/images/748738cc8666dc9de64688e3797d13ff.png">
<meta property="og:image" content="https://blog.sheep0.top/images/dac7d6c449ad2a4cb1ad096f8cbb2b60.png">
<meta property="article:published_time" content="2025-09-30T14:30:45.794Z">
<meta property="article:modified_time" content="2025-09-30T14:30:45.794Z">
<meta property="article:author" content="Sheep0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://blog.sheep0.top/images/04e1aa31e0fe35d358b00f54380e70b3.png">
  
  
  
  <title>page.title - Sheep0&#39;s blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />





<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.sheep0.top","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 60vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="page.title"></span>
          
        </div>

        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      <div class="container nopadding-x-md">
        <div id="board"
          >
          
          <div class="container">
            <div class="row">
              <div class="col-12 col-md-10 m-auto">
                

<article class="page-content">
  <h3 id="一、Tokenization"><a href="#一、Tokenization" class="headerlink" title="一、Tokenization"></a>一、Tokenization</h3><p>将原始文本拆分成一个个token</p>
<h4 id="1-字粒度"><a href="#1-字粒度" class="headerlink" title="1. 字粒度"></a>1. 字粒度</h4><p>对于英语：按字母分割</p>
<p>对于中文：按字分割</p>
<p>优点：词表小</p>
<p>缺点：丢失了大量词汇的语义信息与边界信息、难以训练、对计算也会带来压力（输入输出变多）</p>
<h4 id="2-词粒度"><a href="#2-词粒度" class="headerlink" title="2. 词粒度"></a>2. 词粒度</h4><p>对于英语等拉丁语言：按空格切分</p>
<p>对于中文：前（后）向最大匹配法、最短路径分词法、基于 N-gram 的统计词频分词法</p>
<h5 id="1-前向最大匹配法"><a href="#1-前向最大匹配法" class="headerlink" title="(1) 前向最大匹配法"></a>(1) 前向最大匹配法</h5><p>选定最大匹配长度，例如2（可以更长），根据词典从最长开始匹配，只要词典中有这个词就匹配成功。</p>
<p>eg：他说的确实在理。先匹配“他说”，词典中没有这个词，开始匹配“他”，有这个词，断句。匹配“说的”，词典里没有这个词，匹配“说”，然后断句。最终匹配成果“他 说 的确 实在 理。”</p>
<p>可以看到，这种tokenization办法无法解决歧义的问题，N的选取以及前向后向的选取对结果影响很大。</p>
<h5 id="2-最短路径分词法"><a href="#2-最短路径分词法" class="headerlink" title="(2) 最短路径分词法"></a>(2) 最短路径分词法</h5><p>将每个字分隔开构造词图，每个路径上都有一个数字代表权重，其中搜索词库中存在的词语构建短路径，即权重更小，最后寻找最短路径为所求分词结果。</p>
<img src="04e1aa31e0fe35d358b00f54380e70b3.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:60%;" />

<p>本例的分词结果为：他 说 的确 实在 理</p>
<p>可以发现是对前一种匹配方式的小优化，仍存在相同的问题，存在歧义。</p>
<h5 id="3-基于N-gram-的统计词频分词法"><a href="#3-基于N-gram-的统计词频分词法" class="headerlink" title="(3) 基于N-gram 的统计词频分词法"></a>(3) 基于N-gram 的统计词频分词法</h5><p>与最短路径分词法类似，只不过权重取值该为了n-gram假设中的条件概率。</p>
<img src="5d0e6ceac151b265fb6f9eb622edcd45.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:70%;" />

<h5 id="（4）优缺点"><a href="#（4）优缺点" class="headerlink" title="（4）优缺点"></a>（4）优缺点</h5><p>优点：能很好保留词的边界信息，保留词汇的含义。</p>
<p>缺陷：需要构造的词典过大、低频词无法充分训练、对于英文中一个词语多种形态没有良好的适应性（look、looks、looking需要分为三个词来训练）</p>
<h5 id="3-subword"><a href="#3-subword" class="headerlink" title="3. subword"></a>3. subword</h5><p>目前最常用的tokenization方法，主要有四种方法，分别为BPE、WordPiece、Unigram、SentencePiece</p>
<h5 id="1-BPE"><a href="#1-BPE" class="headerlink" title="(1) BPE"></a>(1) BPE</h5><p>全程Byte Piar Encoding，字节对编码</p>
<p><img src="/f5ca8bd92ac8fcc749ed9ff70118632c.png" srcset="/img/loading.gif" lazyload alt="截图"></p>
<p>即先将所有单词拆分为字母粒度，例如 l o w </w>: 5，	单词low就被分为3个字符，接着再选连续出现频率最高的两个字符，将其合并并且添加到词表中，合并后的字符再后续添加中视为一个字符。例如第三轮添加est，实际上是因为第二轮往词库里加了es之后，将es视为一个整体，再找连续的情况下est出现的概率最高。一直如此循环直到达到设定的词库数目上限为止。</p>
<img src="1fbdd0a92d847ea07dddc63f1d97cd72.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:70%;" />

<p>构建完词典后，用前向最大匹配来进行编码，从而实现分词。</p>
<p>BBPE原理类似，只不过最开始不是按字符划分，而是按Byte划分。缺点就是有可能编码产生一些中间态，编码产生的序列长度可能会更长。好处是可以处理多语言。</p>
<h5 id="（2）WordPiece"><a href="#（2）WordPiece" class="headerlink" title="（2）WordPiece"></a>（2）WordPiece</h5><p>与BPE算法类似，只不过不是基于最高的频数（例如刚刚上述例子，取得是最高的es，因为es组合出现的次数最多），而取得是最高的score值，其中$score(e,s)&#x3D;\frac{count(es)}{count(e)*count(s)}$，这样可以缓解语料库中不同语句的不平衡。</p>
<h5 id="（3）Unigram"><a href="#（3）Unigram" class="headerlink" title="（3）Unigram"></a>（3）Unigram</h5><p>前两者相当于对词典做加法，而Unigram是对词典做减法，从一整个大的词典开始（包括所有subword，当然提前设定了最大subword的字符数）逐个删除某些subword。减法的方法是利用unigram语言模型（认为subword之间是独立的），对每个删除了subword后的语言模型loss进行评分，由于优秀的subword可以使得loss更小，所以删除后loss减少（更多情况下会是loss增加的最少），代表这个待删除的subword是糟糕的，因为如果删除的是优秀的subword会使得loss显著增大。每一次删除会删除一定比例的subword，而不是只删除一个。注意不删除单个字符，避免出现OOV。</p>
<p>loss就是似然值的负对数和，详见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/716907570">https://zhuanlan.zhihu.com/p/716907570</a></p>
<img src="24533fde8728304e988456cd0d070cdf.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:70%;" />

<br/>

<h5 id="（4）SentencePiece"><a href="#（4）SentencePiece" class="headerlink" title="（4）SentencePiece"></a>（4）SentencePiece</h5><p>前三者存在两个问题：无法逆转、推理时需要提前切分。例如对于“Hello Word!”的分词与“Hello World !”的分词是等价的。</p>
<p>SentencePiece采取以下方法解决问题：输入全转为Unicode、空格被处理为特定的token(元符号’_’，U+2581)、直接处理raw文本 </p>
<p>处理后再用BPE或者Unigram来训练分词。</p>
<p>缺陷：处理短文本效果可能不好，可能会出现奇怪的分词；可能会出现将空格(也就是’_’)与其他单词混合起来的奇怪分词；对整体性分词性能较差（”北京大学“分成“北京” 大“ ”学“）</p>
<h3 id="二、词向量"><a href="#二、词向量" class="headerlink" title="二、词向量"></a>二、词向量</h3><p>将输入的token数字化</p>
<h4 id="1-词袋法"><a href="#1-词袋法" class="headerlink" title="1. 词袋法"></a>1. 词袋法</h4><p>实际上词袋法是统计一个句子中不同token的次数，并<strong>将整个句子转化为一个向量</strong>，仅存储了每个词语是否出现（或者出现的次数），无法记录语义信息与位置信息。</p>
<img src="601f006b098e0b189dbed624bf0a1fbe.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:35%;" />

<h4 id="2-tf-idf"><a href="#2-tf-idf" class="headerlink" title="2. tf-idf"></a>2. tf-idf</h4><p>考虑到了不同词的<strong>稀有度</strong>不同，稀有度是在语料库中统计求出$log(\frac{总文档数}{含有这个词的文档数})$来反应这个词的稀有度，将最后得到的<strong>稀有性</strong>乘上句子中出现的<strong>词频</strong>，最为最终的句向量。可以用来提取关键词</p>
<img src="10df3d75326e08b21120e9abe81ec092.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:35%;" />

<img src="7f9dc8d5dcce86802cd09fdb8cfab813.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:40%;" />

<h4 id="3-Word-Embedding"><a href="#3-Word-Embedding" class="headerlink" title="3. Word Embedding"></a>3. Word Embedding</h4><p>x为独热编码，将x右乘一个矩阵Q，得出的向量就是x的词向量。</p>
<img src="de3203e8c626fb32bc05c8cc1f15fc6b.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:50%;" />

<p>上述示例实际上是提取了右边这个矩阵第四行的值（一一对应的表查询）。这样就能让词向量具备语义关系，只需要训练好右边的这个矩阵就行。</p>
<h4 id="4-Word2Vec"><a href="#4-Word2Vec" class="headerlink" title="4. Word2Vec"></a>4. Word2Vec</h4><p>用于训练词嵌入矩阵Q的模型</p>
<p><strong>模型的输出其实不是很重要</strong>，训练好的Word2Vec模型往往具备优秀的Q。模型：softmax(W(xQ)+b)，没有激活函数。</p>
<p>（1）CBOW：给出一个词的<strong>上下文</strong>，来得到这个词。其实就是挖词填空，用一个模型来预测这个空。</p>
<p>（2）Skip-gram：给出一个词，预测这个词的上下文。</p>
<p>缺点：难以训练出一词多义</p>
<h4 id="5-ELMo（Embedding-from-Language-Models）"><a href="#5-ELMo（Embedding-from-Language-Models）" class="headerlink" title="5. ELMo（Embedding from Language Models）"></a>5. ELMo（Embedding from Language Models）</h4><p>也是专门做词向量的模型，可以解决Word2Vec中词向量一词多义问题。</p>
<p> 不只是训练一个Q矩阵，还会把上下文信息融入Q矩阵。 </p>
<img src="8fb17bf5d3312a80d7c56861846ed6a3.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:60%;" />

<p>E是已经乘一个预训练的Q得到的词向量， 然后用两层LSTM（超参数）拼接上下文信息，左边可以提取上文信息，右边可以提取下文信息。 </p>
<p>利用上下文语义信息能对原本的词向量进行调整，进而对一词多义的状况进行调整。例如苹果公司与苹果水果，Apple往往与科技的上下文相关，例如手机，公司等，而水果苹果往往与吃相关。</p>
<br/>

<h3 id="三、Attention注意力机制"><a href="#三、Attention注意力机制" class="headerlink" title="三、Attention注意力机制"></a>三、Attention注意力机制</h3><p>会把注意力聚焦到更重要的事物上，dot(Q,K)V，自注意机制，就是QKV同源；交叉注意力机制，一方提供Q，另一方提供KV</p>
<h4 id="1-自注意机制"><a href="#1-自注意机制" class="headerlink" title="1. 自注意机制"></a>1. 自注意机制</h4><p>$softmax(\frac{dot(Q,K^T)}{\sqrt{d_k}})V$，$d_k$是词向量维度，假设独立分布的$q_i$，$k_i$均值为0，方差为1，那么$dot(q,v)&#x3D;\Sigma_{i&#x3D;1}^{d_k}q_i*k_i$均值为0，方差变为$d_k$，会导致进入softmax的值过于大，故采用scaled Dot-Product Attention除掉$\sqrt{d_k}$</p>
<p><img src="92d1d2d0c1dec2c007b79660bd006b70.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:40%;" /><img src="fcc0195f38f9c59efbad3575ca8840c9.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:40%;" /></p>
<h4 id="2-掩码自注意力机制"><a href="#2-掩码自注意力机制" class="headerlink" title="2. 掩码自注意力机制"></a>2. 掩码自注意力机制</h4><p>masked self-attention</p>
<p>解决训练过程中的问题： 实际上让<strong>训练过程可以并行计算</strong>，即一次性能训练一整个句子。</p>
<p><img src="887e6854a1bf5d78850a613f773eecdb.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:50%;" />   <img src="cdb4fb94d99db597f35c0a67be6b9d54.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:55%;" />  </p>
<h4 id="3-多头自注意力机制"><a href="#3-多头自注意力机制" class="headerlink" title="3.多头自注意力机制"></a>3.多头自注意力机制</h4><p>multi-head self-attention</p>
<p>类比CNN中一层中用多个卷积核(channels 数)，多头个数用h表示，一般取h&#x3D;8。</p>
<p>会用一个W对多头输出的多个y进行处理</p>
<img src="aa319f8e3ace183e660e4642abf5ddb7.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:40%;" />

<h4 id="4-位置编码"><a href="#4-位置编码" class="headerlink" title="4. 位置编码"></a>4. 位置编码</h4><p>attention机制不包含位置信息，即打乱一句话，对词向量没影响。</p>
<p>在embedding和attention之间加一个向量，pos代表位置，i代表维度，i从0开始</p>
<img src="cb7380500a802c0d94b93d1605fd799c.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:40%;" />

<p>位置编码这样的构造即可保证绝对的位置信息，也可保证<strong>相对的位置信息</strong></p>
<img src="b176a81741ba6549ea2f1908584bbbd7.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:40%;" />

<h4 id="5-Layer-Normalize"><a href="#5-Layer-Normalize" class="headerlink" title="5.Layer Normalize"></a>5.Layer Normalize</h4><p>norm：变成均值为0，方差为1的分布。$x&#x3D;\frac{x-u}{\sigma}$，其中u为均值，$\sigma$为标准差。训练时取小批量的均值和标准差。训练结束后取整个数据的均值和标准差用于推理。训练时还会把学习两个参数，将均值和方差变到合适的一个分布。能<strong>加速收敛，提高训练稳定性</strong></p>
<p>BN：对<strong>一个特征</strong>进行norm，CNN就是对<strong>不同channels</strong>进行norm。缺点：<strong>对变长数据效果不佳</strong>。优点：训练时不同的minibatch会有不同的分布，可以为整体的训练添加随机性，避免过拟合。<strong>适用于批量大小较大的情况</strong>，尤其是在图像处理任务中。BN能够有效地减少内部协变量偏移，加速模型的训练过程，并提高模型的泛化能力。</p>
<p>抹杀了不同特征之间的大小关系，强行将不同特征缩放至同一个分布（适合CNN，因为RGB三个值都位于(0,255)；保留了不同样本的特征之间的大小关系。（适合CNN，保留了不同图片的颜色深浅不同）</p>
<p>LN：对<strong>一个batch</strong>进行norm，CNN就是对每张图片（每个batch）进行norm。优点：不依赖batch和seq影响，<strong>能适应不同长度的输入,在批量较小的时候效果优于BN。</strong></p>
<p>抹杀了不同样本之间的大小关系，每个样本的整体取值都比较平均（不适合CNN，会使得一张深红色的图片与浅红色的图片无异）；保留了同一样本不同特征之间的大小关系。（适合Transformer，保留了pos位置信息）</p>
<img src="45e0593eae6b9813290bc1a589ba34a1.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:50%;" />

<p>上图是李沐老师所讲的ln与bn的差异，需要往LayerNorm里传入二维张量。若传入数字，则仅对一个词的d_model作归一化，如下图所示：</p>
<img src="bb90ed7e104f1a65b120fd135ad70521.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:50%;" />

<h3 id="四、Transformer"><a href="#四、Transformer" class="headerlink" title="四、Transformer"></a>四、Transformer</h3><p>seq2seq模型，即序列到序列</p>
<img src="46e67535c49be6125525d986521f2b03.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:70%;" />

<p>推理：</p>
<img src="90a8eb7cae25f02eb8ebceafac55d57e.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:50%;" />

<img src="748738cc8666dc9de64688e3797d13ff.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:50%;" />

<br/>

<p>交叉注意力层中，Encoder提供KV，Decoder提供Q</p>
<p>Transformer在推理输出时，存在巨大的性能浪费：以机器翻译为例，将我是一名学生翻译为I am a student。transformer中decoder输入I am a，这一轮的输出是am a student。前面的”am a”被重复推理，实际上只输出了一个student作为有效的输出。</p>
<h3 id="五、Inductive-Biases（归纳偏置）"><a href="#五、Inductive-Biases（归纳偏置）" class="headerlink" title="五、Inductive Biases（归纳偏置）"></a>五、Inductive Biases（归纳偏置）</h3><img src="dac7d6c449ad2a4cb1ad096f8cbb2b60.png" srcset="/img/loading.gif" lazyload alt="截图" style="zoom:50%;" />

<p>无结构先验：任意两个位置的输入都可以做注意力，导致在小数据集上容易过拟合。</p>
<p>排列等变性：本身不区分输入元素的位置，需要人为添加位置编码</p>
<p>意味着CNN、RNN、GNN利用biases可以保留其泛化能力，而Transformer不需要biases</p>


  

</article>



              </div>
            </div>
          </div>
        </div>
      </div>
    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
