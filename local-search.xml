<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>【Plan】记点目标</title>
    <link href="/2025/10/20/%E3%80%90Plan%E3%80%91%E8%AE%B0%E7%82%B9%E7%9B%AE%E6%A0%87/"/>
    <url>/2025/10/20/%E3%80%90Plan%E3%80%91%E8%AE%B0%E7%82%B9%E7%9B%AE%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<h2 id="1-2025-10-20-10-31"><a href="#1-2025-10-20-10-31" class="headerlink" title="1. 2025.10.20-10.31"></a>1. 2025.10.20-10.31</h2><p>（1）速通C++语法；（2）基于C++过一遍数据结构；（3）探索C++开发的RoadMap，为下一阶段的计划做好规划</p><p>碎碎念：焦虑了好久，算是看开了店。科研是<strong>准研究生</strong>才能接触的事，低绩点的人不应该奢望，还没考上研的人也不应该触碰，除非缺人干苦力。干脆放空自己，学一点C++开发准备找实习，顺便过一遍数据结构。一切应该都还来得及。</p><p><img src="/images%5C%E5%88%86%E6%B8%85%E8%99%9A%E6%8B%9F%E5%92%8C%E7%8E%B0%E5%AE%9E%E5%90%A7.jpg" alt="放弃幻想"></p>]]></content>
    
    
    
    <tags>
      
      <tag>杂谈</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【杂谈】想到什么就记点什么</title>
    <link href="/2025/10/08/%E3%80%90%E6%9D%82%E8%B0%88%E3%80%91%E6%83%B3%E5%88%B0%E4%BB%80%E4%B9%88%E5%B0%B1%E8%AE%B0%E7%82%B9%E4%BB%80%E4%B9%88/"/>
    <url>/2025/10/08/%E3%80%90%E6%9D%82%E8%B0%88%E3%80%91%E6%83%B3%E5%88%B0%E4%BB%80%E4%B9%88%E5%B0%B1%E8%AE%B0%E7%82%B9%E4%BB%80%E4%B9%88/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Lokr训练过拟合问题"><a href="#1-Lokr训练过拟合问题" class="headerlink" title="1. Lokr训练过拟合问题"></a>1. Lokr训练过拟合问题</h2><p>上次把面部和服饰的词全删了，直接用触发词来涵盖整个角色形象。使得训练出的模型过于僵硬，基本上是训练集的图片的直接复刻，表情僵硬（手机自拍挡脸）练死了，相关概念过拟合，无法触发多人图。</p><p>现在的思路是使用完整的caption，让模型能学会每一个精细的概念与描述。若想要复刻人物形象，再用人物相关的形象描述词来控制。包装成给用户使用时，可以用LLM或脚本来提前预设人物外貌相关的描述词，在实际推理的时候补在用户所选用的触发词的后面，可保证用户使用时体验与先前一致。</p><p>至于多人出图，好像大家用SD绘图时基本上都是生成单人图像。如果要多人图像，可以先用SD生成对应的各个角色的图片，再用相关Edit模型来混在一起，最后出成品图片给用户。这种任务拆分和执行训练一个agent应该能做，不过agent怎么搞还没有思路。</p><h2 id="2-多模态相关"><a href="#2-多模态相关" class="headerlink" title="2. 多模态相关"></a>2. 多模态相关</h2><p>LLM发展太快了，给人带来了“智能”感，最近速通了李沐老师的GPT与Bert论文精读，感觉LLM约等于改进的TransformerDecoder+无标号大样本预训练+指令微调+强化学习，架构如果都是基于Decoder的话，剩下的无非是想办法创造出高质量数据集和设计损失函数（个人观点，可能有点武断；最新的模型好像也类似这样，没有仔细了解，平常新闻听说的创新好像也就是专家模型、思维链、强化学习什么的）。</p><p>如果LLM真的是“智能”的话，他的内部应该是有一套自己的语义空间的，那么做多模态只要让“输入”和“输出”都能被LLM内部的语义空间理解就行。自然而然的想法就是直接做“对齐”，用图像文本对来训练，把图片用图像编码器(CNN或者Vit)处理成向量，让他与文本对齐。这样的缺陷就是图片对于LLM而言似乎是直接被等同于一段文字了，对齐的描述性文字无法用文字描述图片的所有信息，因而导致会漏掉一些信息，从而不够智能。例如GPT数手指，如果做对齐的时候不包含有几根手指的描述，图像编码器没学到手指数目与图片的对应关系，那么输出的结果就是瞎蒙（幻觉）。给人的感觉就是图片编码器太呆了，无法灵活地“注意到”用户需要的信息。图片编码器基本上都是在图片分类上做预训练，我感觉像是这个分类任务让他的特征提取变得很呆，如果能用到LLM训练的那一套新东西或许能更好点。(更武断了，应该多收集一些多模态目前遇到的其他问题再做判断，数物品这种问题用语义分割或目标检测再算格子好像也能直接解决)</p><p>至于交叉注意力的那套方案，一开始脑补的训练成本很高，基本上要重新训练LLM的关键层，就没怎么多想了。</p><h2 id="3-未来计划"><a href="#3-未来计划" class="headerlink" title="3. 未来计划"></a>3. 未来计划</h2><p>前一阵子看完了<a href="https://book.douban.com/subject/37451502/">AI共生指南：技术探索与人文思考(豆瓣)</a>（基本上在讲故事哈哈），里面关于强化学习的相关内容还是挺感兴趣的。用AI写强化学习，解决一些现实问题还是太酷了，未来打算稍微学一点，然后借助AI coding搭一个项目试试。</p><p>还有就是后面关于Apple公司的AI赋能有点感染到我了，书中的边端小模型似乎确实很有价值，特别是最近看到了一个蒸馏的32B模型，似乎很强，想再多了解一下。不过这个优先级应该最低，离我太远了。</p><p>幻觉的相关研究到时候也了解一下，DeepSeekR1的幻觉太离谱了，他的所有“人性化”的表现在我眼里都是幻觉，像是训练过程中被逼迫地服从用户指令、满足用户需求的成果。例如<a href="https://b23.tv/UmQqudF">DeepSeek“彩蛋”与“资深用户”-哔哩哔哩</a>，这种做法会误导用户，理应让AI知道什么东西他“不知道”，特别是一些知识性的问题，先入为主的误导真的很麻烦！</p><p>被大佬推荐了一本书，稍微看了一下还是太哲学了，抽空挑点感兴趣的看看得了。<a href="https://book.douban.com/subject/26889398/">认知、模型与表征 (豆瓣)</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>杂谈</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文速通省流】Bert&amp;GPT3</title>
    <link href="/2025/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%80%9F%E9%80%9A%E7%9C%81%E6%B5%81%E3%80%91Bert&amp;GPT3/"/>
    <url>/2025/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%80%9F%E9%80%9A%E7%9C%81%E6%B5%81%E3%80%91Bert&amp;GPT3/</url>
    
    <content type="html"><![CDATA[<h1 id="一、BERT"><a href="#一、BERT" class="headerlink" title="一、BERT"></a>一、BERT</h1><p>Word2Vec 使用训练好的模型来抽取词语、句子的特征，但只能提取比较基本的特征，忽略了时序信息；语言模型只能单向的提取前文的信息，用于后文推理，更多用于生成。</p><p>而这两者面对新的下游任务时，往往只能作为Embedding层等，用于<strong>提取初级特征</strong>，后续还要设计新的网络架构，<strong>从零训练</strong>来适应下游任务。</p><p>BERT想做的是直接设计成一个预训练好的模型，之后应对下游任务只需要在小的数据集上进行微调，之后只需补充一些容易训练的输出层(例如MLP+softmax)即可适应新的下游任务。</p><p>预训练的多层双向Transformer-Encoder{多头自注意力+FFN}</p><ol><li>Layers &#x3D; 12, H &#x3D; 768, A(头数) &#x3D; 12，模型量110M</li><li>L &#x3D; 24, H &#x3D; 1024, A &#x3D; 16，模型量340M</li></ol><h2 id="1-输入"><a href="#1-输入" class="headerlink" title="1. 输入"></a>1. 输入</h2><p>样本是一个句子对，既可以是QA，也可以是相邻的两个句子。</p><p>句子的第一个token一定是[CLS]，两个句子中间是[SEP]。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://img.cdn1.vip/i/68e6a0ffbcaae_1759944959.webp"          style="max-width: 430px; width: 100%; height: auto; border-radius: 8px;" />  </div></div><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://img.cdn1.vip/i/68e6a0ffbed3f_1759944959.webp"          style="max-width: 600px; width: 100%; height: auto; border-radius: 8px;" />  </div></div><p>每个token相当于是三个值的直接相加，第一个是Embedding，第二相当于bias，两个句子的bias各不同，第三个是可学习的位置编码。实际上三者都可以在预训练中被学习优化。</p><h2 id="2-训练阶段"><a href="#2-训练阶段" class="headerlink" title="2.训练阶段"></a>2.训练阶段</h2><h3 id="2-1-预训练"><a href="#2-1-预训练" class="headerlink" title="2.1 预训练"></a>2.1 预训练</h3><p>预训练分为两个任务，第一个任务阶段是做带有掩码的语言模型(Masked Language Model, MLM)，对于输入的文本，除了两个标志，剩下的token有15%的概率被处理，作为被预测的token，其中80%概率这个token会变成[MASK]，10%的概率被替换成词表中的一个随机词汇，10%的概率不做变化，只是用于预测。</p><p>预训练的第二个任务，下一句子预测(Next Sentence Prediction, NSP)。对于句子对的学习，有50%的概率两个句子对是上下文且相邻的句子(即两者有关联)，有50%的概率后者被替换为一个上下文无关的另一个句子。将&lt;cls&gt;对应的输出放在一个全连接层做二分类。</p><p>数据集最好选用文本类型，而不是杂散的文章。</p><p>如果只是将Bert当成高级Embedding模型（例如用于RAG），NSP可能由于过于简单，而可能会影响模型性能。</p><h3 id="2-2-微调"><a href="#2-2-微调" class="headerlink" title="2.2 微调"></a>2.2 微调</h3><p>微调阶段<strong>Bert的权重也会被更新</strong>。</p><p>（1）句子分类：将句子的第一个token，即&lt;cls&gt;输出的向量与一个全连接层分类。</p><p>（2）命名实体识别，即识别一个词元是不是人名、机构名、品牌名、位置等：将除了&lt;cls&gt;、&lt;sep&gt;等特殊词元的其他普通词都放到一个全连接层中，做二分类&#x2F;多分类。</p><p>（3）问题回答，给定一个问题和描述文字，找到一个片段进行回答。即做“阅读理解”，从片段中找到所需要的答案：问题和描述文字分开，将描述文字的每个词进行三分类，判断他是不是答案的“开始、结束、或者都不是“。</p><br/><h1 id="二、GPT"><a href="#二、GPT" class="headerlink" title="二、GPT"></a>二、GPT</h1><h2 id="1-GPT1"><a href="#1-GPT1" class="headerlink" title="1.GPT1"></a>1.GPT1</h2><p>利用Transformer中的解码器，用于nsp，采用半监督学习。</p><p>用了12层解码器，dim&#x3D;768</p><h3 id="1-1-预训练"><a href="#1-1-预训练" class="headerlink" title="1.1 预训练"></a>1.1 预训练</h3><p>使用语言模型的损失函数，最大化窗口为k的似然值。损失函数为L1</p><h3 id="1-2-监督微调"><a href="#1-2-监督微调" class="headerlink" title="1.2 监督微调"></a>1.2 监督微调</h3><p>拿最后一层最后一个词的输出，放入一层全连接层+softmax做分类（交叉熵损失为L2）。但损失函数取$L2+\lambda L1$时更优。</p><h3 id="1-3-迁移"><a href="#1-3-迁移" class="headerlink" title="1.3 迁移"></a>1.3 迁移</h3><p>将子任务表达成 1.2 中的形式，即一串序列和一个标号。</p><p>（1）分类：将输入前后分别加上&lt;start&gt;和&lt;extract&gt;作为序列，标号即为分类的标号。输出再用一个线性层做微调即可。</p><p>（2）蕴含，类似于bert做的阅读理解，给出一段话+一个设问，输出对&#x2F;错&#x2F;不知道：分别加入&lt;start&gt;、&lt;delim&gt;和&lt;extract&gt;，再加一个线性层，做三分类。</p><p>（3）相似，判断两个文本是否相似，可用于关键词检索：由于这是一个对称问题，所以构造了两个类似于情况(2)的文本，只是两个文本一前一后，经过Encoder的输出进行相加，最后进入线性层，进行二分类。</p><p>（4）选择题：给出一个问题和多个答案，选择其中一个答案：类似情况(2)的构造，有多少个选项就构造多少个句子对，每个QA都经过完整的Decoder+Linear，最后用Softmax选最优选项。</p><h2 id="2-GPT2"><a href="#2-GPT2" class="headerlink" title="2. GPT2"></a>2. GPT2</h2><p>设计了四个模型，最大的48层，d&#x3D;1600，1.5B大模型，和GPT1类似的架构，但疑似没怎么打过Bert，所以提出了Zero-Shot的概念。GPT2使用无监督学习，且在不进行微调的情况下，仍然能适应下游任务。</p><p>预训练的时候没有微调时期的特殊符号，所以预训练阶段的数据输入形式要能与下游任务做匹配，即引入Prompt。只要模型足够强大，就能理解prompt，且prompt就是以自然语言的形式展现，在广泛的数据集中可能天生就带有类似的描述。 </p><h2 id="3-GPT3"><a href="#3-GPT3" class="headerlink" title="3. GPT3"></a>3. GPT3</h2><p> 做few-shot而不做zero-shot，175B大模型，下游任务不做梯度更新和微调。 </p><p>问题：1、每次用户输入都要给few shot；2、如果手里有比较多的样本，无法全部都放进去，因为模型的上下文能力有限。</p><p>局限：1、长文本生成很弱；2、语言模型只能往前看；3、每个词都是均匀的预测，无法抓到重点，在虚词的学习中可能消耗很多时长；4、样本有效性比较弱；5、训练消耗很多算力；6、不可解释；7、没做安全绕过防御。</p>]]></content>
    
    
    
    <tags>
      
      <tag>论文速通省流</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Transformer-zero-to-all</title>
    <link href="/2025/10/02/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Transformer-zero-to-all/"/>
    <url>/2025/10/02/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Transformer-zero-to-all/</url>
    
    <content type="html"><![CDATA[<h2 id="一、Tokenization"><a href="#一、Tokenization" class="headerlink" title="一、Tokenization"></a>一、Tokenization</h2><p>将原始文本拆分成一个个token</p><h3 id="1-字粒度"><a href="#1-字粒度" class="headerlink" title="1. 字粒度"></a>1. 字粒度</h3><p>对于英语：按字母分割</p><p>对于中文：按字分割</p><p>优点：词表小</p><p>缺点：丢失了大量词汇的语义信息与边界信息、难以训练、对计算也会带来压力（输入输出变多）</p><h3 id="2-词粒度"><a href="#2-词粒度" class="headerlink" title="2. 词粒度"></a>2. 词粒度</h3><p>对于英语等拉丁语言：按空格切分</p><p>对于中文：前（后）向最大匹配法、最短路径分词法、基于 N-gram 的统计词频分词法</p><h4 id="1-前向最大匹配法"><a href="#1-前向最大匹配法" class="headerlink" title="(1) 前向最大匹配法"></a>(1) 前向最大匹配法</h4><p>选定最大匹配长度，例如2（可以更长），根据词典从最长开始匹配，只要词典中有这个词就匹配成功。</p><p>eg：他说的确实在理。先匹配“他说”，词典中没有这个词，开始匹配“他”，有这个词，断句。匹配“说的”，词典里没有这个词，匹配“说”，然后断句。最终匹配成果“他 说 的确 实在 理。”</p><p>可以看到，这种tokenization办法无法解决歧义的问题，N的选取以及前向后向的选取对结果影响很大。</p><h4 id="2-最短路径分词法"><a href="#2-最短路径分词法" class="headerlink" title="(2) 最短路径分词法"></a>(2) 最短路径分词法</h4><p>将每个字分隔开构造词图，每个路径上都有一个数字代表权重，其中搜索词库中存在的词语构建短路径，即权重更小，最后寻找最短路径为所求分词结果。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/04e1aa31e0fe35d358b00f54380e70b3.png.webp"          style="max-width: 600px; width: 100%; height: auto; border-radius: 8px;" />  </div> </div><p>本例的分词结果为：他 说 的确 实在 理</p><p>可以发现是对前一种匹配方式的小优化，仍存在相同的问题，存在歧义。</p><h4 id="3-基于N-gram-的统计词频分词法"><a href="#3-基于N-gram-的统计词频分词法" class="headerlink" title="(3) 基于N-gram 的统计词频分词法"></a>(3) 基于N-gram 的统计词频分词法</h4><p>与最短路径分词法类似，只不过权重取值该为了n-gram假设中的条件概率。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/5d0e6ceac151b265fb6f9eb622edcd45.png.webp"          style="max-width: 700px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 --></div><h4 id="4-优缺点"><a href="#4-优缺点" class="headerlink" title="(4) 优缺点"></a>(4) 优缺点</h4><p>优点：能很好保留词的边界信息，保留词汇的含义。</p><p>缺陷：需要构造的词典过大、低频词无法充分训练、对于英文中一个词语多种形态没有良好的适应性（look、looks、looking需要分为三个词来训练）</p><h3 id="3-subword"><a href="#3-subword" class="headerlink" title="3. subword"></a>3. subword</h3><p>目前最常用的tokenization方法，主要有四种方法，分别为BPE、WordPiece、Unigram、SentencePiece</p><h4 id="1-BPE"><a href="#1-BPE" class="headerlink" title="(1) BPE"></a>(1) BPE</h4><p>全程Byte Piar Encoding，字节对编码</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/f5ca8bd92ac8fcc749ed9ff70118632c.png.webp"          style="max-width: 500px; width: 100%; height: auto; border-radius: 8px;" />  </div></div><p>即先将所有单词拆分为字母粒度，例如 l o w </w>: 5，单词low就被分为3个字符，接着再选连续出现频率最高的两个字符，将其合并并且添加到词表中，合并后的字符再后续添加中视为一个字符。例如第三轮添加est，实际上是因为第二轮往词库里加了es之后，将es视为一个整体，再找连续的情况下est出现的概率最高。一直如此循环直到达到设定的词库数目上限为止。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/1fbdd0a92d847ea07dddc63f1d97cd72.png.webp"          style="max-width: 500px; width: 100%; height: auto; border-radius: 8px;" />  </div></div><p>构建完词典后，用前向最大匹配来进行编码，从而实现分词。</p><p>BBPE原理类似，只不过最开始不是按字符划分，而是按Byte划分。缺点就是有可能编码产生一些中间态，编码产生的序列长度可能会更长。好处是可以处理多语言。</p><h4 id="（2）WordPiece"><a href="#（2）WordPiece" class="headerlink" title="（2）WordPiece"></a>（2）WordPiece</h4><p>与BPE算法类似，只不过不是基于最高的频数（例如刚刚上述例子，取得是最高的es，因为es组合出现的次数最多），而取得是最高的score值，其中$score(e,s)&#x3D;\frac{count(es)}{count(e)*count(s)}$，这样可以缓解语料库中不同语句的不平衡。</p><h4 id="（3）Unigram"><a href="#（3）Unigram" class="headerlink" title="（3）Unigram"></a>（3）Unigram</h4><p>前两者相当于对词典做加法，而Unigram是对词典做减法，从一整个大的词典开始（包括所有subword，当然提前设定了最大subword的字符数）逐个删除某些subword。减法的方法是利用unigram语言模型（认为subword之间是独立的），对每个删除了subword后的语言模型loss进行评分，由于优秀的subword可以使得loss更小，所以删除后loss减少（更多情况下会是loss增加的最少），代表这个待删除的subword是糟糕的，因为如果删除的是优秀的subword会使得loss显著增大。每一次删除会删除一定比例的subword，而不是只删除一个。注意不删除单个字符，避免出现OOV。</p><p>loss就是似然值的负对数和，详见<a href="https://zhuanlan.zhihu.com/p/716907570">https://zhuanlan.zhihu.com/p/716907570</a></p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/24533fde8728304e988456cd0d070cdf.png.webp"          style="max-width: 600px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 --></div><h4 id="（4）SentencePiece"><a href="#（4）SentencePiece" class="headerlink" title="（4）SentencePiece"></a>（4）SentencePiece</h4><p>前三者存在两个问题：无法逆转、推理时需要提前切分。例如对于“Hello Word!”的分词与“Hello World !”的分词是等价的。</p><p>SentencePiece采取以下方法解决问题：输入全转为Unicode、空格被处理为特定的token(元符号’_’，U+2581)、直接处理raw文本 </p><p>处理后再用BPE或者Unigram来训练分词。</p><p>缺陷：处理短文本效果可能不好，可能会出现奇怪的分词；可能会出现将空格(也就是’_’)与其他单词混合起来的奇怪分词；对整体性分词性能较差（”北京大学“分成“北京” 大“ ”学“）</p><h2 id="二、词向量"><a href="#二、词向量" class="headerlink" title="二、词向量"></a>二、词向量</h2><p>将输入的token数字化</p><h3 id="1-词袋法"><a href="#1-词袋法" class="headerlink" title="1. 词袋法"></a>1. 词袋法</h3><p>实际上词袋法是统计一个句子中不同token的次数，并<strong>将整个句子转化为一个向量</strong>，仅存储了每个词语是否出现（或者出现的次数），无法记录语义信息与位置信息。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/601f006b098e0b189dbed624bf0a1fbe.png.webp"          style="max-width: 600px; width: 80%; height: auto; border-radius: 8px;" />  </div></div><h3 id="2-tf-idf"><a href="#2-tf-idf" class="headerlink" title="2. tf-idf"></a>2. tf-idf</h3><p>考虑到了不同词的<strong>稀有度</strong>不同，稀有度是在语料库中统计求出$log(\frac{总文档数}{含有这个词的文档数})$来反应这个词的稀有度，将最后得到的<strong>稀有性</strong>乘上句子中出现的<strong>词频</strong>，最为最终的句向量。可以用来提取关键词</p><div align="center" style="margin: 20px 0;">  <div style="display: flex; justify-content: center; gap: 20px; align-items: flex-start; margin-bottom: 10px;">    <img src="https://aliyun.sheep0.top/images/10df3d75326e08b21120e9abe81ec092.png.webp"           style="max-width: 380px; height: auto; border-radius: 8px;" />    <img src="https://aliyun.sheep0.top/images/7f9dc8d5dcce86802cd09fdb8cfab813.png.webp"           style="max-width: 400px; height: auto; border-radius: 8px;" />  </div></div><h3 id="3-Word-Embedding"><a href="#3-Word-Embedding" class="headerlink" title="3. Word Embedding"></a>3. Word Embedding</h3><p>x为独热编码，将x右乘一个矩阵Q，得出的向量就是x的词向量。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/de3203e8c626fb32bc05c8cc1f15fc6b.png.webp"          style="max-width: 450px; width: 100%; height: auto; border-radius: 8px;" />  </div></div><p>上述示例实际上是提取了右边这个矩阵第四行的值（一一对应的表查询）。这样就能让词向量具备语义关系，只需要训练好右边的这个矩阵就行。</p><h3 id="4-Word2Vec"><a href="#4-Word2Vec" class="headerlink" title="4. Word2Vec"></a>4. Word2Vec</h3><p>用于训练词嵌入矩阵Q的模型</p><p><strong>模型的输出其实不是很重要</strong>，训练好的Word2Vec模型往往具备优秀的Q。模型：softmax(W(xQ)+b)，没有激活函数。</p><p>（1）CBOW：给出一个词的<strong>上下文</strong>，来得到这个词。其实就是挖词填空，用一个模型来预测这个空。</p><p>（2）Skip-gram：给出一个词，预测这个词的上下文。</p><p>缺点：难以训练出一词多义</p><h3 id="5-ELMo（Embedding-from-Language-Models）"><a href="#5-ELMo（Embedding-from-Language-Models）" class="headerlink" title="5. ELMo（Embedding from Language Models）"></a>5. ELMo（Embedding from Language Models）</h3><p>也是专门做词向量的模型，可以解决Word2Vec中词向量一词多义问题。</p><p> 不只是训练一个Q矩阵，还会把上下文信息融入Q矩阵。 </p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/8fb17bf5d3312a80d7c56861846ed6a3.png.webp"          style="max-width: 500px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">  </div></div><p>E是已经乘一个预训练的Q得到的词向量， 然后用两层LSTM（超参数）拼接上下文信息，左边可以提取上文信息，右边可以提取下文信息。 </p><p>利用上下文语义信息能对原本的词向量进行调整，进而对一词多义的状况进行调整。例如苹果公司与苹果水果，Apple往往与科技的上下文相关，例如手机，公司等，而水果苹果往往与吃相关。</p><h2 id="三、Attention注意力机制"><a href="#三、Attention注意力机制" class="headerlink" title="三、Attention注意力机制"></a>三、Attention注意力机制</h2><p>会把注意力聚焦到更重要的事物上，dot(Q,K)V，自注意机制，就是QKV同源；交叉注意力机制，一方提供Q，另一方提供KV</p><h3 id="1-自注意机制"><a href="#1-自注意机制" class="headerlink" title="1. 自注意机制"></a>1. 自注意机制</h3><p>$softmax(\frac{dot(Q,K^T)}{\sqrt{d_k}})V$，$d_k$是词向量维度，假设独立分布的$q_i$，$k_i$均值为0，方差为1，那么$dot(q,v)&#x3D;\Sigma_{i&#x3D;1}^{d_k}q_i*k_i$均值为0，方差变为$d_k$，会导致进入softmax的值过于大，故采用scaled Dot-Product Attention除掉$\sqrt{d_k}$</p><div align="center" style="margin: 20px 0;">  <div style="display: flex; justify-content: center; gap: 20px; align-items: flex-start; margin-bottom: 10px;">    <img src="https://aliyun.sheep0.top/images/fcc0195f38f9c59efbad3575ca8840c9.png.webp"           style="max-width: 410px; height: auto; border-radius: 8px;" />    <img src="https://aliyun.sheep0.top/images/92d1d2d0c1dec2c007b79660bd006b70.png.webp"           style="max-width: 400px; height: auto; border-radius: 8px;" />  </div>  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px;">  </div></div><h3 id="2-掩码自注意力机制"><a href="#2-掩码自注意力机制" class="headerlink" title="2. 掩码自注意力机制"></a>2. 掩码自注意力机制</h3><p>masked self-attention</p><p>解决训练过程中的问题： 实际上让<strong>训练过程可以并行计算</strong>，即一次性能训练一整个句子。</p><div align="center" style="margin: 20px 0;">  <div style="display: flex; justify-content: center; gap: 20px; align-items: flex-start; margin-bottom: 10px;">    <img src="https://aliyun.sheep0.top/images/887e6854a1bf5d78850a613f773eecdb.png.webp"           style="max-width: 400px; height: auto; border-radius: 8px;" />    <img src="https://aliyun.sheep0.top/images/cdb4fb94d99db597f35c0a67be6b9d54.png.webp"           style="max-width: 240px; height: auto; border-radius: 8px;" />  </div>  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px;">  </div></div><h3 id="3-多头自注意力机制"><a href="#3-多头自注意力机制" class="headerlink" title="3.多头自注意力机制"></a>3.多头自注意力机制</h3><p>multi-head self-attention</p><p>类比CNN中一层中用多个卷积核(channels 数)，多头个数用h表示，一般取h&#x3D;8。</p><p>会用一个W对多头输出的多个y进行处理。</p><p>实际在实现的时候是将hidden(或者说dim，一般都是最后一个维度)缩小head倍，contact到batch中，即batch增大head倍。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/aa319f8e3ace183e660e4642abf5ddb7.png.webp"          style="max-width: 400px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">  </div></div><h3 id="4-位置编码"><a href="#4-位置编码" class="headerlink" title="4. 位置编码"></a>4. 位置编码</h3><p>attention机制不包含位置信息，即打乱一句话，对词向量没影响。</p><p>在embedding和attention之间加一个向量，pos代表位置，i代表维度，i从0开始</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/cb7380500a802c0d94b93d1605fd799c.png.webp"          style="max-width: 400px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">  </div></div><p>位置编码这样的构造即可保证绝对的位置信息，也可保证<strong>相对的位置信息</strong></p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/b176a81741ba6549ea2f1908584bbbd7.png.webp"          style="max-width: 500px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">  </div></div>###  5.Layer Normalize<p>norm：变成均值为0，方差为1的分布。$x&#x3D;\frac{x-u}{\sigma}$，其中u为均值，$\sigma$为标准差。训练时取小批量的均值和标准差。训练结束后取整个数据的均值和标准差用于推理。训练时还会把学习两个参数，将均值和方差变到合适的一个分布。能<strong>加速收敛，提高训练稳定性</strong></p><p>BN：对<strong>一个特征</strong>进行norm，CNN就是对<strong>不同channels</strong>进行norm。缺点：<strong>对变长数据效果不佳</strong>。优点：训练时不同的minibatch会有不同的分布，可以为整体的训练添加随机性，避免过拟合。<strong>适用于批量大小较大的情况</strong>，尤其是在图像处理任务中。BN能够有效地减少内部协变量偏移，加速模型的训练过程，并提高模型的泛化能力。</p><p>抹杀了不同特征之间的大小关系，强行将不同特征缩放至同一个分布（适合CNN，因为RGB三个值都位于(0,255)；保留了不同样本的特征之间的大小关系。（适合CNN，保留了不同图片的颜色深浅不同）</p><p>LN：对<strong>一个batch</strong>进行norm，CNN就是对每张图片（每个batch）进行norm。优点：不依赖batch和seq影响，<strong>能适应不同长度的输入,在批量较小的时候效果优于BN。</strong></p><p>抹杀了不同样本之间的大小关系，每个样本的整体取值都比较平均（不适合CNN，会使得一张深红色的图片与浅红色的图片无异）；保留了同一样本不同特征之间的大小关系。（适合Transformer，保留了pos位置信息）</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/45e0593eae6b9813290bc1a589ba34a1.png.webp"          style="max-width: 280px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">  </div></div><p>上图是李沐老师所讲的ln与bn的差异，需要往LayerNorm里传入二维张量。若传入数字，则仅对一个词的d_model作归一化，如下图所示：</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/bb90ed7e104f1a65b120fd135ad70521.png.webp"          style="max-width: 400px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">  </div></div><h2 id="四、Transformer"><a href="#四、Transformer" class="headerlink" title="四、Transformer"></a>四、Transformer</h2><p>seq2seq模型，即序列到序列</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/46e67535c49be6125525d986521f2b03.png.webp"          style="max-width: 330px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">  </div></div><h3 id="推理与训练"><a href="#推理与训练" class="headerlink" title="推理与训练"></a>推理与训练</h3><div align="center" style="margin: 20px 0;">  <div style="display: flex; justify-content: center; gap: 20px; align-items: flex-start; margin-bottom: 10px;">    <img src="https://aliyun.sheep0.top/images/90a8eb7cae25f02eb8ebceafac55d57e.png.webp"           style="max-width: 430px; height: auto; border-radius: 8px;" />    <img src="https://aliyun.sheep0.top/images/748738cc8666dc9de64688e3797d13ff.png.webp"           style="max-width: 430px; height: auto; border-radius: 8px;" />  </div>  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px;">  </div></div>交叉注意力层中，Encoder提供KV，Decoder提供Q<p>Transformer在推理输出时，存在巨大的性能浪费：以机器翻译为例，将我是一名学生翻译为I am a student。transformer中decoder输入I am a，这一轮的输出是am a student。前面的”am a”被重复推理，实际上只输出了一个student作为有效的输出。</p><h2 id="五、Inductive-Biases（归纳偏置）"><a href="#五、Inductive-Biases（归纳偏置）" class="headerlink" title="五、Inductive Biases（归纳偏置）"></a>五、Inductive Biases（归纳偏置）</h2><p>不同模型在处理对应的数据时，会提前假设在该任务上的数据所满足的一定的条件，被称作归纳偏置。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://aliyun.sheep0.top/images/dac7d6c449ad2a4cb1ad096f8cbb2b60.png.webp"          style="max-width: 300px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">  </div></div><p>无结构先验：任意两个位置的输入都可以做注意力，导致在小数据集上容易过拟合。</p><p>排列等变性：本身不区分输入元素的位置，需要人为添加位置编码</p><p>意味着CNN、RNN、GNN利用biases可以保留其泛化能力，而Transformer不需要biases</p><h2 id="六、参考文献"><a href="#六、参考文献" class="headerlink" title="六、参考文献"></a>六、参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/444774532">Tokenization</a></p><p><a href="https://zhuanlan.zhihu.com/p/716907570">Unigram分词</a></p><p>还有李沐老师的教程与Mingsheng Long老师的课件（），还有b站大学的一些视频（）</p><p>笔记里面夹杂着一点自己的理解，很有可能有错误之处或表达不完整之处。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【多模态玩具】多模态大模型的训练和推理</title>
    <link href="/2025/09/29/%E3%80%90%E5%A4%9A%E6%A8%A1%E6%80%81%E7%8E%A9%E5%85%B7%E3%80%91%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86/"/>
    <url>/2025/09/29/%E3%80%90%E5%A4%9A%E6%A8%A1%E6%80%81%E7%8E%A9%E5%85%B7%E3%80%91%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="一、什么是多模态？"><a href="#一、什么是多模态？" class="headerlink" title="一、什么是多模态？"></a>一、什么是多模态？</h2><p>通过融合多种数据模态（例如图片、音频、视频、文本等）来训练模型，从而提高模型的感知与理解能力，实现跨模态的信息交互与融合。</p><p>文本模态的表示：文本模态的表示方法有多种，如独热表示、低维空间表示（如通过神经网络模型学习得到的转换矩阵将单词或字映射到语义空间中）、词袋表示及其衍生出的n-grams词袋表示等。目前，主流的文本表示方法是预训练文本模型，如BERT。</p><p>视觉模态的表示：视觉模态分为图像模态和视频模态。图像模态的表示主要通过卷积神经网络（CNN）实现，如LeNet-5、AlexNet、VGG、GoogLeNet、ResNet等。视频模态的表示则结合了图像的空间属性和时间属性，通常由CNN和循环神经网络（RNN）或长短时记忆网络（LSTM）等模型共同处理。</p><p>声音模态的表示：声音模态的表示通常涉及音频信号的预处理、特征提取和表示学习等步骤，常用的模型包括深度神经网络（DNN）、卷积神经网络（CNN）和循环神经网络（RNN）等。</p><h2 id="二、多模态数据如何融合？"><a href="#二、多模态数据如何融合？" class="headerlink" title="二、多模态数据如何融合？"></a>二、多模态数据如何融合？</h2><p>以图像-文本为例，常见的融合思路有两种：</p><h3 id="1-统一嵌入解码器架构方法"><a href="#1-统一嵌入解码器架构方法" class="headerlink" title="1. 统一嵌入解码器架构方法"></a>1. 统一嵌入解码器架构方法</h3><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://img.cdn1.vip/i/68d9ddc6bf4d9_1759108550.webp"          style="max-width: 350px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">    图1 统一嵌入解码器架构  </div></div><p>统一嵌入解码器架构方法采用单一解码器模型，类似于未经修改的 LLM 架构（例如 GPT-2 或 Llama 3.2）。在这种方法中，图像被Encoder转换为与原始文本 token 具有相同嵌入大小的 token，从而允许 LLM 在连接后同时处理文本和图像输入 token。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://img.cdn1.vip/i/68d9de78aea5e_1759108728.webp"          style="max-width: 400px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">    图2 图像编码器内部架构  </div></div><p>图像编码器架构如上图所示，就是提取传统的预训练视觉转换器（ViT），舍去其最后的分类全连接层。Image Encoder中的Linear projection（即线性投影模块），实际上是由全连接层将256维向量的图像块投影至768维向量，实际上用卷积层会更为容易，两者也是等价的。</p><p>在提取Image Encoder之后，我们还会增加一层投影模块，将图片信息最终投影至与文本词向量输出维度相匹配。就类似于文本的向量化，只是多了一步投影对齐到同一维度。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://img.cdn1.vip/i/68d9defaf1b81_1759108858.webp"          style="max-width: 500px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">    图3 类比文本编码器  </div></div><p>最后直接将处理过后的图像信息与文本信息相拼接，输入llm中进行推理即可。</p><h3 id="2-跨模态注意力架构方法"><a href="#2-跨模态注意力架构方法" class="headerlink" title="2. 跨模态注意力架构方法"></a>2. 跨模态注意力架构方法</h3><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://img.cdn1.vip/i/68d9df7d4742a_1759108989.webp"          style="max-width: 450px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">    图4 跨模态注意力架构  </div></div><p>跨模态注意力架构中，则是将图片信息经过编码与投影后，加在llm中的多头注意力层中，通过交叉注意力机制进行处理。在交叉注意力机制中，图像信息提供K和V，文本信息提供Q。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://img.cdn1.vip/i/68d9dfc1a949e_1759109057.webp"         style="max-width: 450px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">    图5 交叉注意力机制  </div></div><h2 id="三、如何训练？"><a href="#三、如何训练？" class="headerlink" title="三、如何训练？"></a>三、如何训练？</h2><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://img.cdn1.vip/i/68d9e003e82fc_1759109123.webp"          style="max-width: 600px; width: 100%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">    图6 多模态训练部分  </div></div><p>与传统纯文本 LLM 的开发类似，多模态 LLM 的训练也包含两个阶段：预训练和指令微调。然而，与从零开始不同，多模态 LLM 的训练通常以一个经过预训练和指令微调的纯文本 LLM 作为基础模型。</p><p>对于图像编码器，常用CLIP编码器，并且通常在整个训练过程中保持不变（冻结参数）。在预训练阶段冻结 LLM 部分也很常见，只专注于训练投影器——一个线性层或一个小型多层感知器。鉴于投影器的学习能力有限，通常仅包含一到两层，LLM 通常会在多模态指令微调（阶段 2）期间解冻，以便进行更全面的更新。然而，在基于交叉注意力机制的模型（方法 B）中，交叉注意力层在整个训练过程中都是解冻的。</p><p>统一嵌入解码器架构（方法 A）通常更容易实现，因为它不需要对 LLM 架构本身进行任何修改。</p><p>跨模态注意力架构（方法 B）通常被认为具有更高的计算效率，因为它不会用额外的图像标记使输入上下文过载，而是稍后在交叉注意力层中引入它们。此外，如果 LLM 参数在训练期间保持冻结，则此方法可以保持原始 LLM 的纯文本性能。</p><h2 id="四、实操"><a href="#四、实操" class="headerlink" title="四、实操"></a>四、实操</h2><p>参考LLaVa的训练方式，文本编码器选用Qwen2.5-0.5B-Instruct，图像编码器用Siglip-base-patch16-224，以问答形式的图像-文本对为训练数据。根据课设的要求，采用 <a href="https://github.com/UCSC-VLAA/MedTrinity-25M">MedTrinity-25M</a>的Demo为数据集，仅进行预训练，不进行指令微调。</p><p>由于原数据集仅包含图像与对应的Caption，因此人工添加描述性的Question，采用与LLaVa预训练阶段同款的预设问题。整个数据集有16w张图像文本对，跑一个batch其实差不多就过拟合了。</p><p>训练代码参考：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> io<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> pyarrow.parquet <span class="hljs-keyword">as</span> pq<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span>, <span class="hljs-type">Dict</span>, <span class="hljs-type">Any</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (<br>    PreTrainedModel, <br>    PretrainedConfig, <br>    AutoTokenizer, <br>    AutoModelForCausalLM,<br>    AutoProcessor, <br>    AutoModel,<br>    Trainer, <br>    TrainingArguments<br>)<br><span class="hljs-keyword">from</span> transformers.modeling_outputs <span class="hljs-keyword">import</span> CausalLMOutputWithPast<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># 定义模型配置类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VLMConfig</span>(<span class="hljs-title class_ inherited__">PretrainedConfig</span>):<br>    model_type = <span class="hljs-string">&quot;vlm_model&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        llm_model_path=<span class="hljs-string">&#x27;model/Qwen2.5-0.5B-Instruct&#x27;</span>,</span><br><span class="hljs-params">        vision_model_path=<span class="hljs-string">&#x27;model/siglip-base-patch16-224&#x27;</span>,</span><br><span class="hljs-params">        freeze_vision_model=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        freeze_llm_model=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        image_pad_num=<span class="hljs-number">49</span>,  <span class="hljs-comment"># 添加image_pad_num参数</span></span><br><span class="hljs-params">        **kwargs</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-variable language_">self</span>.vision_model_path = vision_model_path<br>        <span class="hljs-variable language_">self</span>.llm_model_path = llm_model_path<br>        <span class="hljs-variable language_">self</span>.freeze_vision_model = freeze_vision_model<br>        <span class="hljs-variable language_">self</span>.freeze_llm_model = freeze_llm_model<br>        <span class="hljs-variable language_">self</span>.image_pad_num = image_pad_num<br>        <span class="hljs-built_in">super</span>().__init__(**kwargs)<br><br><span class="hljs-comment"># 定义多模态模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VLM</span>(<span class="hljs-title class_ inherited__">PreTrainedModel</span>):<br>    config_class = VLMConfig<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__(config)<br>        <span class="hljs-variable language_">self</span>.config = config<br>        <br>        <span class="hljs-comment"># 加载视觉编码器</span><br>        <span class="hljs-variable language_">self</span>.vision_model = AutoModel.from_pretrained(<span class="hljs-variable language_">self</span>.config.vision_model_path)<br>        <span class="hljs-variable language_">self</span>.processor = AutoProcessor.from_pretrained(<span class="hljs-variable language_">self</span>.config.vision_model_path)<br>        <br>        <span class="hljs-comment"># 加载语言模型</span><br>        <span class="hljs-variable language_">self</span>.llm_model = AutoModelForCausalLM.from_pretrained(<span class="hljs-variable language_">self</span>.config.llm_model_path)<br>        <span class="hljs-variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-variable language_">self</span>.config.llm_model_path)<br>        <br>        <span class="hljs-comment"># 确保特殊token存在</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;&lt;|image_pad|&gt;&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.tokenizer.get_vocab():<br>            <span class="hljs-variable language_">self</span>.tokenizer.add_special_tokens(&#123;<span class="hljs-string">&#x27;additional_special_tokens&#x27;</span>: [<span class="hljs-string">&#x27;&lt;|image_pad|&gt;&#x27;</span>]&#125;)<br>            <span class="hljs-variable language_">self</span>.llm_model.resize_token_embeddings(<span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.tokenizer))<br>        <br>        <span class="hljs-comment"># 图像特征维度与编码器隐藏维度的关系</span><br>        vision_hidden_size = <span class="hljs-variable language_">self</span>.vision_model.config.vision_config.hidden_size<br>        llm_hidden_size = <span class="hljs-variable language_">self</span>.llm_model.config.hidden_size<br>        <br>        <span class="hljs-comment"># SiGLIP输出是(batch_size, 196, hidden_size)，需要映射到更少的tokens</span><br>        <span class="hljs-comment"># 定义投影层 - 先将196个token压缩到49个token，然后投影到LLM维度</span><br>        <span class="hljs-variable language_">self</span>.linear1 = nn.Linear(vision_hidden_size * <span class="hljs-number">4</span>, llm_hidden_size)  <span class="hljs-comment"># 196 -&gt; 49 (每4个合并)</span><br>        <span class="hljs-variable language_">self</span>.linear2 = nn.Linear(llm_hidden_size, llm_hidden_size)<br>        <br>        <span class="hljs-comment"># 冻结模型参数</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.config.freeze_vision_model:<br>            <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.vision_model.parameters():<br>                param.requires_grad = <span class="hljs-literal">False</span><br>                <br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.config.freeze_llm_model:<br>            <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.llm_model.parameters():<br>                param.requires_grad = <span class="hljs-literal">False</span><br>        <br>    <span class="hljs-comment"># def forward(self, input_ids, attention_mask=None, labels, pixel_values):</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_ids, labels, pixel_values, attention_mask=<span class="hljs-literal">None</span></span>):<br>        <br>        <span class="hljs-comment"># 获取文本嵌入</span><br>        text_embeds = <span class="hljs-variable language_">self</span>.llm_model.get_input_embeddings()(input_ids)<br>        <br>        <span class="hljs-comment"># 处理图像嵌入</span><br>        <span class="hljs-keyword">if</span> pixel_values <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># print(f&quot;pix is here&quot;, pixel_values.shape)</span><br>            <span class="hljs-comment"># norm = (pixel_values - 1).norm()</span><br>            <span class="hljs-comment"># print(f&quot;x is&quot;,norm)</span><br>            image_embeds = <span class="hljs-variable language_">self</span>.vision_model.vision_model(pixel_values).last_hidden_state<br>            b, s, d = image_embeds.shape<br>            <span class="hljs-comment"># 压缩图片tokens: (b, 196, d) --&gt; (b, 49, d*4)</span><br>            image_embeds = image_embeds.view(b, -<span class="hljs-number">1</span>, d*<span class="hljs-number">4</span>)<br>            image_features = <span class="hljs-variable language_">self</span>.linear2(F.silu(<span class="hljs-variable language_">self</span>.linear1(image_embeds)))<br>            <br>            <span class="hljs-comment"># 将图像特征合并到输入嵌入中</span><br>            text_embeds = text_embeds.to(image_features.dtype)<br>            inputs_embeds = <span class="hljs-variable language_">self</span>.merge_input_ids_with_image_features(image_features, text_embeds, input_ids)<br>        <span class="hljs-keyword">else</span>:<br>            inputs_embeds = text_embeds<br>        <br>        <span class="hljs-comment"># 通过语言模型</span><br>        outputs = <span class="hljs-variable language_">self</span>.llm_model(<br>            inputs_embeds=inputs_embeds,<br>            attention_mask=attention_mask,<br>            labels=labels,<br>            return_dict=<span class="hljs-literal">True</span><br>        )<br>        <br>        <span class="hljs-keyword">return</span> outputs<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">merge_input_ids_with_image_features</span>(<span class="hljs-params">self, image_features, inputs_embeds, input_ids</span>):<br>        <span class="hljs-comment"># 找到所有&lt;|image_pad|&gt;的位置</span><br>        image_pad_id = <span class="hljs-variable language_">self</span>.tokenizer.convert_tokens_to_ids(<span class="hljs-string">&#x27;&lt;|image_pad|&gt;&#x27;</span>)<br>        batch_indices, image_indices = torch.where(input_ids == image_pad_id)<br>        <br>        <span class="hljs-comment"># 获取唯一的batch_indices，以处理每个样本</span><br>        unique_batch_indices = batch_indices.unique()<br>        <br>        <span class="hljs-keyword">for</span> batch_idx <span class="hljs-keyword">in</span> unique_batch_indices:<br>            <span class="hljs-comment"># 获取当前批次的所有image_pad位置</span><br>            pad_positions = image_indices[batch_indices == batch_idx]<br>            <br>            <span class="hljs-comment"># 确保我们有足够的image_pad位置</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(pad_positions) &gt;= <span class="hljs-variable language_">self</span>.config.image_pad_num:<br>                <span class="hljs-comment"># 只使用前image_pad_num个位置</span><br>                pad_positions = pad_positions[:<span class="hljs-variable language_">self</span>.config.image_pad_num]<br>                <br>                <span class="hljs-comment"># 将image_features的每一行放入对应的image_pad位置</span><br>                <span class="hljs-keyword">for</span> i, pos <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(pad_positions):<br>                    <span class="hljs-keyword">if</span> i &lt; image_features.shape[<span class="hljs-number">1</span>]:<br>                        inputs_embeds[batch_idx, pos] = image_features[batch_idx, i]<br>        <br>        <span class="hljs-keyword">return</span> inputs_embeds<br><br><span class="hljs-comment"># 定义数据集类</span><br><span class="hljs-comment"># 不同的数据集的组织形式可能不同，作者给出的组织方式也可能与实际情况不足</span><br><span class="hljs-comment"># 最好先试试再再写Dateset！</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MedicalImageDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data_dir, tokenizer, processor, image_pad_num=<span class="hljs-number">49</span></span>):<br>        <span class="hljs-variable language_">self</span>.data_dir = data_dir<br>        <span class="hljs-variable language_">self</span>.tokenizer = tokenizer<br>        <span class="hljs-variable language_">self</span>.processor = processor<br>        <span class="hljs-variable language_">self</span>.image_pad_num = image_pad_num<br>        <span class="hljs-variable language_">self</span>.data = []<br>        <span class="hljs-variable language_">self</span>.instructions = [<br>            <span class="hljs-string">&quot;Describe the following image in detail&quot;</span>,<br>            <span class="hljs-string">&quot;Provide a detailed description of the given image&quot;</span>,<br>            <span class="hljs-string">&quot;Give an elaborate explanation of the image you see&quot;</span>,<br>            <span class="hljs-string">&quot;Share a comprehensive rundown of the presented image&quot;</span>,<br>            <span class="hljs-string">&quot;Offer a thorough analysis of the image&quot;</span>,<br>            <span class="hljs-string">&quot;Explain the various aspects of the image before you&quot;</span>,<br>            <span class="hljs-string">&quot;Clarify the contents of the displayed image with great detail&quot;</span>,<br>            <span class="hljs-string">&quot;Characterize the image using a well-detailed description&quot;</span>,<br>            <span class="hljs-string">&quot;Break down the elements of the image in a detailed manner&quot;</span>,<br>            <span class="hljs-string">&quot;Walk through the important details of the image&quot;</span>,<br>            <span class="hljs-string">&quot;Portray the image with a rich, descriptive narrative&quot;</span>,<br>            <span class="hljs-string">&quot;Narrate the contents of the image with precision&quot;</span>,<br>            <span class="hljs-string">&quot;Analyze the image in a comprehensive and detailed manner&quot;</span>,<br>            <span class="hljs-string">&quot;Illustrate the image through a descriptive explanation&quot;</span>,<br>            <span class="hljs-string">&quot;Examine the image closely and share its details&quot;</span>,<br>            <span class="hljs-string">&quot;Write an exhaustive depiction of the given image&quot;</span><br>        ]<br>        <br>        <span class="hljs-comment"># 加载所有parquet文件</span><br>        parquet_files = [f <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> os.listdir(data_dir) <span class="hljs-keyword">if</span> f.endswith(<span class="hljs-string">&#x27;.parquet&#x27;</span>)]<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Found <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(parquet_files)&#125;</span> parquet files&quot;</span>)<br>        <br>        <span class="hljs-keyword">for</span> pq_file <span class="hljs-keyword">in</span> tqdm(parquet_files, desc=<span class="hljs-string">&quot;Loading data&quot;</span>):<br>            file_path = os.path.join(data_dir, pq_file)<br>            table = pq.read_table(file_path)<br>            df = table.to_pandas()<br>            <br>            <span class="hljs-keyword">for</span> _, row <span class="hljs-keyword">in</span> df.iterrows():<br>                <span class="hljs-comment"># 检查必要的字段是否存在</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">all</span>(field <span class="hljs-keyword">in</span> row <span class="hljs-keyword">for</span> field <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;image&#x27;</span>, <span class="hljs-string">&#x27;caption&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>]):<br>                    image_bytes = row[<span class="hljs-string">&#x27;image&#x27;</span>][<span class="hljs-string">&#x27;bytes&#x27;</span>] <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(row[<span class="hljs-string">&#x27;image&#x27;</span>], <span class="hljs-built_in">dict</span>) <span class="hljs-keyword">and</span> <span class="hljs-string">&#x27;bytes&#x27;</span> <span class="hljs-keyword">in</span> row[<span class="hljs-string">&#x27;image&#x27;</span>] <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>                    <span class="hljs-variable language_">self</span>.data.append(&#123;<br>                        <span class="hljs-string">&#x27;image&#x27;</span>: image_bytes,  <span class="hljs-comment"># 直接存储二进制图像数据</span><br>                        <span class="hljs-string">&#x27;caption&#x27;</span>: row[<span class="hljs-string">&#x27;caption&#x27;</span>],<br>                        <span class="hljs-string">&#x27;id&#x27;</span>: row[<span class="hljs-string">&#x27;id&#x27;</span>]<br>                    &#125;)<br>                    <br>        <br>        <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Loaded <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(self.data)&#125;</span> image-caption pairs&quot;</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.data)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        item = <span class="hljs-variable language_">self</span>.data[idx]<br>        <br>        <span class="hljs-comment"># 随机选择一个指令</span><br>        instruction = random.choice(<span class="hljs-variable language_">self</span>.instructions)<br>        <br>        <span class="hljs-comment"># 构建提示，添加图像占位符</span><br>        prompt = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;instruction&#125;</span>\n&quot;</span> + <span class="hljs-string">&quot;&lt;|image_pad|&gt;&quot;</span> * <span class="hljs-variable language_">self</span>.image_pad_num<br>        answer = item[<span class="hljs-string">&#x27;caption&#x27;</span>]<br>        <br>        <span class="hljs-comment"># 从二进制数据加载图像</span><br>        <span class="hljs-keyword">try</span>:<br>            image = Image.<span class="hljs-built_in">open</span>(io.BytesIO(item[<span class="hljs-string">&#x27;image&#x27;</span>])).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>            pixel_values = <span class="hljs-variable language_">self</span>.processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values.squeeze(<span class="hljs-number">0</span>)<br>            <span class="hljs-comment"># pixel_values = self.processor(text=None, images=image, return_tensors=&quot;pt&quot;)[&#x27;pixel_values&#x27;]</span><br>            <span class="hljs-comment"># print(f&quot;shape is&quot;, pixel_values.shape)</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(pixel_values) != torch.Tensor:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Wrong!!!!!!!!!!!!!!!!!&quot;</span>) <br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Error loading image <span class="hljs-subst">&#123;item[<span class="hljs-string">&#x27;id&#x27;</span>]&#125;</span>: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>            <span class="hljs-comment"># 创建一个空白图像</span><br>            image = Image.new(<span class="hljs-string">&#x27;RGB&#x27;</span>, (<span class="hljs-number">224</span>, <span class="hljs-number">224</span>), color=<span class="hljs-string">&#x27;white&#x27;</span>)<br>            pixel_values = <span class="hljs-variable language_">self</span>.processor(text=<span class="hljs-literal">None</span>, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&#x27;pixel_values&#x27;</span>]<br>        <br>        <span class="hljs-comment"># 简化的对话格式构建，不依赖chat_template</span><br>        system = <span class="hljs-string">&quot;You are a helpful medical image analysis assistant.&quot;</span><br>        user_message = prompt<br>        assistant_message = answer<br>        <br>        <span class="hljs-comment"># 构建输入文本，使用简单的格式</span><br>        text = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;system&#125;</span>\n\nUser: <span class="hljs-subst">&#123;user_message&#125;</span>\n\nAssistant: <span class="hljs-subst">&#123;assistant_message&#125;</span>&quot;</span><br>        <br>        <span class="hljs-comment"># 分词</span><br>        encoding = <span class="hljs-variable language_">self</span>.tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">512</span>)<br>        input_ids = encoding[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]<br>        attention_mask = encoding[<span class="hljs-string">&quot;attention_mask&quot;</span>][<span class="hljs-number">0</span>]<br>        <br>        <span class="hljs-comment"># 构建标签：找到Assistant部分的开始位置</span><br>        assistant_start = text.find(<span class="hljs-string">&quot;Assistant: &quot;</span>)<br>        <span class="hljs-keyword">if</span> assistant_start != -<span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># 计算Assistant开始前的token数量</span><br>            prefix_text = text[:assistant_start]<br>            prefix_tokens = <span class="hljs-variable language_">self</span>.tokenizer(prefix_text, add_special_tokens=<span class="hljs-literal">False</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]<br>            assistant_pos = <span class="hljs-built_in">len</span>(prefix_tokens)<br>            <span class="hljs-comment"># 可能需要加上特殊token的偏移量</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.tokenizer.bos_token_id <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                assistant_pos += <span class="hljs-number">1</span>  <span class="hljs-comment"># 加上BOS token</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 如果找不到，估算一个位置（大约是前半部分）</span><br>            assistant_pos = <span class="hljs-built_in">len</span>(input_ids) // <span class="hljs-number">2</span><br>            <br>        <span class="hljs-comment"># 构建标签</span><br>        labels = input_ids.clone()<br>        labels[:assistant_pos] = -<span class="hljs-number">100</span>  <span class="hljs-comment"># 用户部分标签设为-100</span><br>        <span class="hljs-comment"># print(f&quot;shape is&quot;, pixel_values.shape)</span><br>        <br>        <span class="hljs-keyword">return</span> &#123;<br>            <span class="hljs-string">&quot;input_ids&quot;</span>: input_ids,<br>            <span class="hljs-string">&quot;attention_mask&quot;</span>: attention_mask,<br>            <span class="hljs-string">&quot;labels&quot;</span>: labels,<br>            <span class="hljs-string">&quot;pixel_values&quot;</span>: pixel_values<br>        &#125;<br><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataCollator</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tokenizer</span>):<br>        <span class="hljs-variable language_">self</span>.tokenizer = tokenizer<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, features</span>):<br>        <span class="hljs-comment"># 确保所有输入都是有效的</span><br>        valid_features = []<br>        <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features:<br>            <span class="hljs-keyword">if</span> f[<span class="hljs-string">&quot;pixel_values&quot;</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">isinstance</span>(f[<span class="hljs-string">&quot;pixel_values&quot;</span>], torch.Tensor):<br>                valid_features.append(f)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Warning: Skipping invalid sample with None or non-tensor pixel_values&quot;</span>)<br>        <br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> valid_features:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;No valid samples in batch!&quot;</span>)<br>        <br>        <span class="hljs-comment"># 正确堆叠张量</span><br>        batch = &#123;<br>            <span class="hljs-string">&quot;input_ids&quot;</span>: torch.stack([f[<span class="hljs-string">&quot;input_ids&quot;</span>] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> valid_features]),<br>            <span class="hljs-string">&quot;attention_mask&quot;</span>: torch.stack([f[<span class="hljs-string">&quot;attention_mask&quot;</span>] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> valid_features]),<br>            <span class="hljs-string">&quot;labels&quot;</span>: torch.stack([f[<span class="hljs-string">&quot;labels&quot;</span>] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> valid_features]),<br>            <span class="hljs-string">&quot;pixel_values&quot;</span>: torch.stack([f[<span class="hljs-string">&quot;pixel_values&quot;</span>] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> valid_features])<br>        &#125;<br>        <br>        <span class="hljs-keyword">return</span> batch<br><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># 设置配置</span><br>    config = VLMConfig(<br>        llm_model_path=<span class="hljs-string">&#x27;model/Qwen2.5-0.5B-Instruct&#x27;</span>,<br>        vision_model_path=<span class="hljs-string">&#x27;model/siglip-base-patch16-224&#x27;</span>,<br>        freeze_vision_model=<span class="hljs-literal">True</span>,<br>        freeze_llm_model=<span class="hljs-literal">True</span>,<br>        image_pad_num=<span class="hljs-number">49</span><br>    )<br>    <br>    <span class="hljs-comment"># 初始化模型</span><br>    model = VLM(config)<br>    tokenizer = AutoTokenizer.from_pretrained(config.llm_model_path)<br>    processor = AutoProcessor.from_pretrained(config.vision_model_path)<br>    <br>    <span class="hljs-comment"># 确保tokenizer有需要的token</span><br>    tokenizer.add_special_tokens(&#123;<span class="hljs-string">&#x27;additional_special_tokens&#x27;</span>: [<span class="hljs-string">&#x27;&lt;|image_pad|&gt;&#x27;</span>]&#125;)<br>    <br>    <span class="hljs-comment"># 确保tokenizer有pad_token</span><br>    <span class="hljs-keyword">if</span> tokenizer.pad_token <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        tokenizer.pad_token = tokenizer.eos_token<br>    <br>    <span class="hljs-comment"># 打印可训练参数数量</span><br>    total_params = <span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters())<br>    trainable_params = <span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters() <span class="hljs-keyword">if</span> p.requires_grad)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;总参数量: <span class="hljs-subst">&#123;total_params&#125;</span>, 可训练参数量: <span class="hljs-subst">&#123;trainable_params&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-comment"># 创建数据集</span><br>    dataset = MedicalImageDataset(<br>        data_dir=<span class="hljs-string">&quot;data/MedTrinity&quot;</span>,<br>        tokenizer=tokenizer,<br>        processor=processor,<br>        image_pad_num=<span class="hljs-number">49</span><br>    )<br>    <br>    <span class="hljs-comment"># 训练参数</span><br>    training_args = TrainingArguments(<br>        output_dir=<span class="hljs-string">&quot;./output&quot;</span>,<br>        num_train_epochs=<span class="hljs-number">1</span>,<br>        per_device_train_batch_size=<span class="hljs-number">8</span>,<br>        gradient_accumulation_steps=<span class="hljs-number">4</span>,<br>        save_steps=<span class="hljs-number">500</span>,<br>        save_total_limit=<span class="hljs-number">2</span>,<br>        learning_rate=<span class="hljs-number">1e-4</span>,<br>        warmup_steps=<span class="hljs-number">500</span>,<br>        logging_dir=<span class="hljs-string">&quot;./logs&quot;</span>,<br>        logging_steps=<span class="hljs-number">50</span>,<br>        eval_strategy=<span class="hljs-string">&quot;no&quot;</span>,<br>        fp16=<span class="hljs-literal">True</span>,<br>        dataloader_num_workers=<span class="hljs-number">4</span>,<br>        report_to=<span class="hljs-string">&quot;tensorboard&quot;</span>,<br>    )<br>   <br>    <span class="hljs-comment"># 初始化训练器</span><br>    trainer = Trainer(<br>        model=model,<br>        args=training_args,<br>        train_dataset=dataset,<br>        data_collator=MyDataCollator(tokenizer),<br>    )<br>    <br>    <span class="hljs-comment"># 开始训练</span><br>    trainer.train()<br>    <br>    <span class="hljs-comment"># 保存模型</span><br>    trainer.save_model(<span class="hljs-string">&quot;./final_model&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练完成，模型已保存到 ./final_model&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br><br></code></pre></td></tr></table></figure><p>推理代码参考：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, AutoProcessor, AutoConfig<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> train <span class="hljs-keyword">import</span> VLMConfig, VLM<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><br>device = <span class="hljs-string">&quot;cuda:0&quot;</span><br>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;model/siglip-base-patch16-224&quot;</span>)<br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;model/Qwen2.5-0.5B-Instruct&#x27;</span>)<br>AutoConfig.register(<span class="hljs-string">&quot;vlm_model&quot;</span>, VLMConfig)<br>AutoModelForCausalLM.register(VLMConfig, VLM)<br><br>pretrain_model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;output/checkpoint-5051&#x27;</span>)<br>pretrain_model.to(device)<br><br><span class="hljs-comment"># sft_model = AutoModelForCausalLM.from_pretrained(&#x27;/home/user/wyf/train_multimodal_from_scratch/save/sft&#x27;)</span><br><span class="hljs-comment"># sft_model.to(device)</span><br><br>pretrain_model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-comment"># sft_model.eval()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">image_input, text_input, max_new_tokens = <span class="hljs-number">256</span>, temperature = <span class="hljs-number">0.0</span>, top_k = <span class="hljs-literal">None</span></span>):<br>    q_text = tokenizer.apply_chat_template([&#123;<span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>:<span class="hljs-string">&#x27;You are a helpful medical assistant.&#x27;</span>&#125;, &#123;<span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>:<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;text_input&#125;</span>\n&lt;image&gt;&#x27;</span>&#125;], \<br>            tokenize=<span class="hljs-literal">False</span>, \<br>            add_generation_prompt=<span class="hljs-literal">True</span>).replace(<span class="hljs-string">&#x27;&lt;image&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;|image_pad|&gt;&#x27;</span>*<span class="hljs-number">49</span>)<br>    input_ids = tokenizer(q_text, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)[<span class="hljs-string">&#x27;input_ids&#x27;</span>]<br>    input_ids = input_ids.to(device)<br>    <span class="hljs-comment"># image = Image.open(image_input).convert(&quot;RGB&quot;)</span><br>    pixel_values = processor(text=<span class="hljs-literal">None</span>, images=image_input).pixel_values<br>    pixel_values = pixel_values.to(device)<br>    eos = tokenizer.eos_token_id<br>    s = input_ids.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">while</span> input_ids.shape[<span class="hljs-number">1</span>] &lt; s + max_new_tokens - <span class="hljs-number">1</span>:  <br>        <span class="hljs-comment"># if mode == &#x27;pretrain&#x27;:</span><br>        model = pretrain_model<br>        <span class="hljs-comment"># else:</span><br>        <span class="hljs-comment">#     model = sft_model</span><br>        inference_res = model(input_ids, <span class="hljs-literal">None</span>, pixel_values)  <br>        logits = inference_res.logits <br>        logits = logits[:, -<span class="hljs-number">1</span>, :] <br><br>        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(input_ids.tolist()[<span class="hljs-number">0</span>]):  <br>            logits[:, token] /= <span class="hljs-number">1.0</span><br><br>        <span class="hljs-keyword">if</span> temperature == <span class="hljs-number">0.0</span>: <br>            _, idx_next = torch.topk(logits, k=<span class="hljs-number">1</span>, dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">else</span>:<br>            logits = logits / temperature  <br>            <span class="hljs-keyword">if</span> top_k <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:  <br>                v, _ = torch.topk(logits, <span class="hljs-built_in">min</span>(top_k, logits.size(-<span class="hljs-number">1</span>)))<br>                logits[logits &lt; v[:, [-<span class="hljs-number">1</span>]]] = -<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;Inf&#x27;</span>) <br><br>            probs = F.softmax(logits, dim=-<span class="hljs-number">1</span>)  <br>            idx_next = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>, generator=<span class="hljs-literal">None</span>)  <br><br>        <span class="hljs-keyword">if</span> idx_next == eos:  <br>            <span class="hljs-keyword">break</span><br><br>        input_ids = torch.cat((input_ids, idx_next), dim=<span class="hljs-number">1</span>)  <br>    <span class="hljs-keyword">return</span> tokenizer.decode(input_ids[:, s:][<span class="hljs-number">0</span>])<br><br><span class="hljs-keyword">with</span> gr.Blocks() <span class="hljs-keyword">as</span> demo:<br>    <span class="hljs-keyword">with</span> gr.Row():<br>        <span class="hljs-comment"># 上传图片</span><br>        <span class="hljs-keyword">with</span> gr.Column(scale=<span class="hljs-number">1</span>):<br>                image_input = gr.Image(<span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;pil&quot;</span>, label=<span class="hljs-string">&quot;选择图片&quot;</span>)<br>        <span class="hljs-keyword">with</span> gr.Column(scale=<span class="hljs-number">1</span>):<br>            <span class="hljs-comment"># mode = gr.Radio([&quot;pretrain&quot;, &quot;sft&quot;], label=&quot;选择模型&quot;)</span><br>            mode = <span class="hljs-string">&#x27;pretrain&#x27;</span><br>            text_input = gr.Textbox(label=<span class="hljs-string">&quot;输入文本&quot;</span>)<br>            <span class="hljs-comment"># text_output = gr.Textbox(label=&quot;输出文本&quot;)</span><br>            text_output = gr.Textbox(<br>                label=<span class="hljs-string">&quot;输出文本&quot;</span>,<br>                lines=<span class="hljs-number">10</span>,         <span class="hljs-comment"># 显示时默认有 10 行高</span><br>                max_lines=<span class="hljs-number">20</span>,     <span class="hljs-comment"># 最多扩展到 20 行（如果内容太多）</span><br>                interactive=<span class="hljs-literal">False</span> <span class="hljs-comment"># 输出一般不可编辑</span><br>            )<br>            generate_button = gr.Button(<span class="hljs-string">&quot;生成&quot;</span>)<br>            generate_button.click(generate, inputs=[image_input, text_input], outputs=text_output)<br>            <br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    demo.launch(share=<span class="hljs-literal">False</span>, server_name=<span class="hljs-string">&quot;0.0.0.0&quot;</span>, server_port=<span class="hljs-number">7891</span>)<br>    <br><br></code></pre></td></tr></table></figure><p>实际推理肯定是过拟合的，就用了大量数据却只训练了一个projection，应付一下课设就先这样不管他了，况且本人没有医疗知识背景，无法对模型推理的结果进行人为判断，也没有找到良好的医疗背景下的指令微调数据集，就先这样了。</p><p>最后把模型输出的英文Caption用外部LLM转为中文，再用脚本转为PDF报告，就能做一个医疗CT图像+文本指令控制输出医疗诊断报告的流，综设这样就完事了。</p><p>参考资料：</p><p><a href="https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html">理论指导</a></p><p><a href="https://github.com/wyf3/llm_related/tree/main/train_multimodal_from_scratch">参考实操项目</a></p><p><a href="https://arxiv.org/pdf/2304.08485">LLaVa’s Papper</a></p><p><a href="https://github.com/UCSC-VLAA/MedTrinity-25M">数据集 MedTrinity-25M</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【炼丹玩具】AI绘图LoKR训练角色概念的尝试</title>
    <link href="/2025/09/24/%E3%80%90%E7%82%BC%E4%B8%B9%E7%8E%A9%E5%85%B7%E3%80%91AI%E7%BB%98%E5%9B%BELoKR%E8%AE%AD%E7%BB%83%E8%A7%92%E8%89%B2%E6%A6%82%E5%BF%B5%E7%9A%84%E5%B0%9D%E8%AF%95/"/>
    <url>/2025/09/24/%E3%80%90%E7%82%BC%E4%B8%B9%E7%8E%A9%E5%85%B7%E3%80%91AI%E7%BB%98%E5%9B%BELoKR%E8%AE%AD%E7%BB%83%E8%A7%92%E8%89%B2%E6%A6%82%E5%BF%B5%E7%9A%84%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h2 id="一-什么是LoKr"><a href="#一-什么是LoKr" class="headerlink" title="一.什么是LoKr"></a>一.什么是LoKr</h2><p>LoKr是微调技术的一种，属于Lycoris，其效果和应用场景类似于LoRA。</p><p>模型的训练实际上是模型参数的改变，利用梯度下降来计算模型参数的变化量$\Delta W$，最后用$W_{new} &#x3D; \Delta W + W_{before}$. </p><p><strong>LoRA</strong> (Low Rank Adaption，低秩微调)大概思想就是，如果我能用一个更好训练的低秩矩阵来替代$\Delta W$，就能实现更好的微调。</p><div align="center" style="margin: 20px 0;">  <!-- 图片 -->  <div style="text-align: center; padding: 10px 12px;">    <img src="https://img.cdn1.vip/i/68d39ef758af2_1758699255.webp"          style="max-width: 600px; width: 40%; height: auto; border-radius: 8px;" />  </div>  <!-- 图注 -->  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px; text-align: center;">图1 LoRA原理图  </div></div><p>如上图所示，实际上LoRA的$\Delta W$用了BA两个矩阵的乘积来表示。</p><p>而LoKr的思想与之类似，两者的具体思想与理论证明本人仍未具体推理学习过，理论上两者都适用于模型的微调(常用于SD绘图，也可用于LLM)，具备参数量小的特点。相较于LoRA，LoKr更容易过拟合。</p><h2 id="二-实操训练LoKr"><a href="#二-实操训练LoKr" class="headerlink" title="二.实操训练LoKr"></a>二.实操训练LoKr</h2><p>在Windows环境下，使用秋叶训练器进行训练。<a href="https://gitee.com/Akegarasu/lora-scripts.git">https://gitee.com/Akegarasu/lora-scripts.git</a></p><p>选用底座模型为SDXL私模，数据集来源为互联网，打算训练mortis和muzimi，mortis大概30多张，muzimi大概20多张，每个epoch每张照片训练10次，训练20个epoch，batch&#x3D;3</p><p>直接总结失败经验：</p><h3 id="1-参数设置错误"><a href="#1-参数设置错误" class="headerlink" title="1.参数设置错误"></a>1.参数设置错误</h3><p>错误地把 <strong>factor</strong> 设置为默认的-1，导致lokr模型参数过小。</p><p>该参数越大训练出的模型越小，实测factor取3时，训练出的模型大小约为1.5GB。</p><h3 id="2-打标问题"><a href="#2-打标问题" class="headerlink" title="2.打标问题"></a>2.打标问题</h3><p>本次训练目标是训练两个角色概念，即用户在正面提示词处输入 mortis 或者 muzimi，即可保证生成对应的角色图像。但在打标过程中，直接用 JoyCaption2 的输出作为打标的文本。其中包含大量对待训练对象的<strong>详细外貌描写语句</strong>，使得最终训练出的模型无法正确理解两个触发词mortis&#x2F;muzimi的包含的图像信息。</p><p>我采取人工重新打标的方式后，一张张看照片，再根据joycaption的输出自行删减修改每一个打标文件。</p><h3 id="3-图像预处理问题"><a href="#3-图像预处理问题" class="headerlink" title="3.图像预处理问题"></a>3.图像预处理问题</h3><p>收集到的初始数据集存在分辨率大小不一，比例不统一等一系列问题。训练过程中训练器能自动识别<strong>不同比例</strong>的图片并将其进行缩放到不同的bucket之中，因此只要保证训练集足够大，图片的比例种类不要太多即可，真正造成困扰的是<strong>分辨率</strong>问题。SDXL训练目标为1024*1024像素，如果输入1920*1080像素的照片，则会将其进行等比例缩放，使得<strong>长边</strong>恰好为1024像素，即1920*1080–&gt;1024*576，而训练器自带的像素缩放工具会导致图像过于模糊。</p><p>我采取的解决方式如下：将所有数据集图像从png–&gt;jpg，用python脚本缩放至长边1024，用SeedVR2放大至1024(此时<strong>短边</strong>为1024，但图片格式被转为了png)，再用python脚本缩放至<strong>长边</strong>1024，并将图像转为jpg。</p><p>经过以上操作的图片，可肉眼可见的发现图像质量有所上升，否则会导致模型学习到模糊的人像。</p><h3 id="3-未知问题（未解决）"><a href="#3-未知问题（未解决）" class="headerlink" title="3.未知问题（未解决）"></a>3.未知问题（未解决）</h3><p>疑似是参数设计错误，我在训练的时候采用的图像预览（即每训练到一定程度，会用指定提示词生成图像），预览出的图像均正常，但实际拿到LoKr模型后，发现只要加载该模型，最终输出的图像都会变成灰蒙蒙的，偶尔能隐约看到一点轮廓。再重新配置完参数，删除数据集的.npz文件，再训练出的模型就没问题。</p><h2 id="三-效果与仍未解决的问题"><a href="#三-效果与仍未解决的问题" class="headerlink" title="三.效果与仍未解决的问题"></a>三.效果与仍未解决的问题</h2><h3 id="1-只能稳定触发单个触发词"><a href="#1-只能稳定触发单个触发词" class="headerlink" title="1.只能稳定触发单个触发词"></a>1.只能稳定触发单个触发词</h3><p>能识别到单人触发词，例如单独的 muzimi 或者单独的 mortis 都能正常输出比较正确的图像，但遇到”mortis and muzimi”双人的照片则表现效果不佳，两人的外貌特征输出有误，往往以第一个角色为主，且或许由于数据集不平等的原因，影响力mortis&gt;muzimi。</p><div align="center" style="margin: 20px 0;">  <div style="display: flex; justify-content: center; gap: 20px; align-items: flex-start; margin-bottom: 10px;">    <img src="https://img.cdn1.vip/i/68d3a1760e5b0_1758699894.webp"           style="max-width: 300px; height: auto; border-radius: 8px;" />    <img src="https://img.cdn1.vip/i/68d3a28ca7421_1758700172.webp"           style="max-width: 300px; height: auto; border-radius: 8px;" />  </div>  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px;">    Prompt:two girls, mortis and muzimi/muzimi and mortis, they look at the camera with guitar in their hand.  </div></div>### 2.打标不完善<p>例如mortis的数据集内包含较多的手机挡脸自拍照，打标过程中并未能很好体现<strong>手机遮脸</strong>这一特征，导致直接用mortis出图可能会出现类似的图。</p><p>例如部分打标文本中存在”serious expression”等包含神态的语句，我在人工打标的过程中没有删除，导致模型可能学习到了奇怪的面部，在生图过程如果不加以面部描述可能导致面部扭曲。</p><p>打标过程使用记事本+图片预览，效率过低。</p><h3 id="3-细节学习有误"><a href="#3-细节学习有误" class="headerlink" title="3.细节学习有误"></a>3.细节学习有误</h3><p>经典的手指学习不完备，可能输出“多手的怪物”。guitar疑似出现过拟合，生成的guitar可能奇奇怪怪不完备。muzimi可能会“长出帽子”。</p><div align="center" style="margin: 20px 0;">  <div style="display: flex; justify-content: center; gap: 20px; align-items: flex-start; margin-bottom: 10px;">    <img src="https://img.cdn1.vip/i/68d391f83ac45_1758695928.webp"           style="max-width: 300px; height: auto; border-radius: 8px;" />    <img src="https://img.cdn1.vip/i/68d3941843779_1758696472.webp"           style="max-width: 300px; height: auto; border-radius: 8px;" />  </div>Prompt:mortis/muzimi, full body, looking at the camera with guitar in her hand  <div style="color: #666; font-size: 0.9em; font-family: 'Helvetica', 'Arial', sans-serif; margin-top: 8px;">]]></content>
    
    
    
    <tags>
      
      <tag>SDXL-LoKr训练</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
